{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pntMn5SI7jaY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "6edd3b11-75f0-4041-af76-2094e80f114f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Buffer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5dd135711ec0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mobservation_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mpolicy_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mvalue_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Buffer' is not defined"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from tensorflow.keras import layers, Model\n",
        "import gym\n",
        "\n",
        "# Create CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 1000\n",
        "steps_per_epoch = 2048\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01\n",
        "clip_ratio = 0.2\n",
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "batch_size = 64\n",
        "render = False\n",
        "\n",
        "# Initialize buffers and optimizers\n",
        "observation_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "buffer = Buffer(observation_dim, size=steps_per_epoch)\n",
        "policy_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "value_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "\n",
        "# Seed for reproducibility\n",
        "seed_generator = tf.random.Generator.from_seed(1337)\n",
        "\n",
        "# Transformer Model Definitions\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(units, activation='relu')(x)\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "def build_transformer_critic(input_shape, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "# Initialize actor and critic models\n",
        "actor = build_transformer_actor(input_shape=(observation_dim,), num_actions=num_actions)\n",
        "critic = build_transformer_critic(input_shape=(observation_dim,))\n",
        "\n",
        "# Function for computing log-probabilities\n",
        "def logprobabilities(logits, actions):\n",
        "    logprobabilities_all = tf.nn.log_softmax(logits, axis=-1)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(actions, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "# Sample action from the actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)  # Actor produces logits\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)  # Sample action\n",
        "    return logits, action\n",
        "\n",
        "# PPO Policy Update (using clipped objective)\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = actor(observation_buffer)  # Actor produces logits\n",
        "        action_probs = tf.nn.softmax(logits)  # Get the action probabilities\n",
        "        current_logprob = logprobabilities(logits, action_buffer)  # Log probability of taken actions\n",
        "\n",
        "        ratio = tf.exp(current_logprob - logprobability_buffer)  # Policy ratio (current / old)\n",
        "        min_advantage = tf.where(\n",
        "            advantage_buffer > 0,\n",
        "            (1 + clip_ratio) * advantage_buffer,\n",
        "            (1 - clip_ratio) * advantage_buffer\n",
        "        )\n",
        "\n",
        "        # PPO clipped objective\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    # Calculate KL divergence for monitoring\n",
        "    kl = tf.reduce_mean(logprobability_buffer - current_logprob)\n",
        "    return kl\n",
        "\n",
        "# PPO Value Function Update (MSE loss)\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_predictions = critic(observation_buffer)  # Critic output\n",
        "        value_loss = tf.reduce_mean(tf.square(return_buffer - value_predictions))  # MSE loss\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "# Discounted cumulative sums for advantages and returns\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "# Buffer Class for storing trajectories\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (\n",
        "            self.observation_buffer,\n",
        "            self.action_buffer,\n",
        "            self.advantage_buffer,\n",
        "            self.return_buffer,\n",
        "            self.logprobability_buffer,\n",
        "        )\n",
        "\n",
        "# Training Loop\n",
        "def train_ppo(agent, env, epochs=1000, steps_per_epoch=2048, train_policy_iterations=80, train_value_iterations=80, target_kl=0.01):\n",
        "    state = env.reset()\n",
        "    episode_return = 0\n",
        "    episode_length = 0\n",
        "    num_episodes = 0\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "\n",
        "        # Iterate over the steps of each epoch\n",
        "        for t in range(steps_per_epoch):\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            # Reshape observation for transformer models\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)  # Sample action from the actor\n",
        "            observation_new, reward, done, _, _ = env.step(action[0].numpy())  # Take step in the environment\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of action from the critic\n",
        "            value_t = critic(observation)  # Get value estimate from the critic\n",
        "            logprobability_t = logprobabilities(logits, action)  # Log-probability for action\n",
        "\n",
        "            # Store data in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # Finish trajectory if terminal state reached\n",
        "            if done or (t == steps_per_epoch - 1):\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))  # Last value from the critic\n",
        "                buffer.finish_trajectory(last_value)  # Finish trajectory\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation, _ = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer = buffer.get()\n",
        "\n",
        "        # Update the policy\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:  # Early stopping based on KL divergence\n",
        "                break\n",
        "\n",
        "        # Update the value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print stats for the epoch\n",
        "        print(f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "# Train PPO Agent on CartPole\n",
        "train_ppo(agent=None, env=env)  # Pass the environment and agent to the training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import gym\n",
        "import scipy.signal\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lam = 0.95  # GAE lambda\n",
        "clip_ratio = 0.2  # PPO clipping ratio\n",
        "epochs = 1000\n",
        "steps_per_epoch = 4000\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01  # Early stopping based on KL divergence\n",
        "\n",
        "# Buffer for storing experiences\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (self.observation_buffer, self.action_buffer, self.advantage_buffer, self.return_buffer, self.logprobability_buffer)\n",
        "\n",
        "\n",
        "# Discounted cumulative sum for rewards\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "# Transformer Actor model\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "# Transformer Critic model\n",
        "def build_transformer_critic(input_shape, units=256, num_heads=2, num_layers=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "\n",
        "# Log-probabilities computation for discrete actions\n",
        "def logprobabilities(logits, a):\n",
        "    logprobabilities_all = tf.math.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "    return logits, action\n",
        "\n",
        "\n",
        "# PPO Policy Gradient Update\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ratio = tf.exp(logprobabilities(actor(observation_buffer), action_buffer) - logprobability_buffer)\n",
        "        min_advantage = tf.where(advantage_buffer > 0, (1 + clip_ratio) * advantage_buffer, (1 - clip_ratio) * advantage_buffer)\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(actor(observation_buffer), action_buffer))\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Value Function Update\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "\n",
        "# Training loop for PPO agent\n",
        "def train_ppo(agent, env, epochs=epochs):\n",
        "    observation_shape = env.observation_space.shape\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Initialize models, optimizers, and buffers\n",
        "    global actor, critic, policy_optimizer, value_optimizer\n",
        "    actor = build_transformer_actor(observation_shape, num_actions)\n",
        "    critic = build_transformer_critic(observation_shape)\n",
        "    policy_optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "    value_optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    buffer = Buffer(observation_shape[0], size=steps_per_epoch)\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "        observation = env.reset()\n",
        "\n",
        "        for t in range(steps_per_epoch):\n",
        "            # Sample action and take a step in the environment\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)\n",
        "            observation_new, reward, done, _, _ = env.step(action.numpy()[0])\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of the action\n",
        "            value_t = critic(observation)\n",
        "            logprobability_t = logprobabilities(logits, action)\n",
        "\n",
        "            # Store experience in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # If done, finish trajectory and reset environment\n",
        "            if done or t == steps_per_epoch - 1:\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "                buffer.finish_trajectory(last_value)\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        (\n",
        "            observation_buffer,\n",
        "            action_buffer,\n",
        "            advantage_buffer,\n",
        "            return_buffer,\n",
        "            logprobability_buffer,\n",
        "        ) = buffer.get()\n",
        "\n",
        "        # Update policy with PPO\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:  # Early stopping based on KL divergence\n",
        "                break\n",
        "\n",
        "        # Update value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print stats for the epoch\n",
        "        print(f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "\n",
        "# Train the PPO agent on CartPole environment\n",
        "train_ppo(agent=None, env=env)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "nfPzbPUl9xzq",
        "outputId": "169b224e-89aa-4837-dd9b-6b6abe33772a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"global_average_pooling1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36m<cell line: 199>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Train the PPO agent on CartPole environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(agent, env, epochs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Initialize models, optimizers, and buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mpolicy_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36mbuild_transformer_actor\u001b[0;34m(input_shape, num_actions, num_heads, num_layers, units)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_last_axis_squeeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import gym\n",
        "import scipy.signal\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lam = 0.95  # GAE lambda\n",
        "clip_ratio = 0.2  # PPO clipping ratio\n",
        "epochs = 1000\n",
        "steps_per_epoch = 4000\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01  # Early stopping based on KL divergence\n",
        "\n",
        "# Initialize episode return and length\n",
        "episode_return = 0\n",
        "episode_length = 0\n",
        "\n",
        "# Buffer for storing experiences\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (self.observation_buffer, self.action_buffer, self.advantage_buffer, self.return_buffer, self.logprobability_buffer)\n",
        "\n",
        "\n",
        "# Discounted cumulative sum for rewards\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "# Transformer Actor model\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Reshape((1, input_shape[0]))(inputs)  # Reshape to (batch_size, 1, features)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pool across the sequence dimension\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "# Transformer Critic model\n",
        "def build_transformer_critic(input_shape, units=256, num_heads=2, num_layers=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Reshape((1, input_shape[0]))(inputs)  # Reshape to (batch_size, 1, features)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pool across the sequence dimension\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "\n",
        "# Log-probabilities computation for discrete actions\n",
        "def logprobabilities(logits, a):\n",
        "    logprobabilities_all = tf.math.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "    return logits, action\n",
        "\n",
        "\n",
        "# PPO Policy Gradient Update\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ratio = tf.exp(logprobabilities(actor(observation_buffer), action_buffer) - logprobability_buffer)\n",
        "        min_advantage = tf.where(advantage_buffer > 0, (1 + clip_ratio) * advantage_buffer, (1 - clip_ratio) * advantage_buffer)\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(actor(observation_buffer), action_buffer))\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Value Function Update\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "\n",
        "# Training loop for PPO agent\n",
        "def train_ppo(agent, env, epochs=epochs):\n",
        "    observation_shape = env.observation_space.shape\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Initialize models, optimizers, and buffers\n",
        "    global actor, critic, policy_optimizer, value_optimizer\n",
        "    actor = build_transformer_actor(observation_shape, num_actions)\n",
        "    critic = build_transformer_critic(observation_shape)\n",
        "    policy_optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "    value_optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    buffer = Buffer(observation_shape[0], size=steps_per_epoch)\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "        observation = env.reset()\n",
        "\n",
        "        for t in range(steps_per_epoch):\n",
        "            # Sample action and take a step in the environment\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)\n",
        "            observation_new, reward, done, info = env.step(action.numpy()[0])\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of the action\n",
        "            value_t = critic(observation)\n",
        "            logprobability_t = logprobabilities(logits, action)\n",
        "\n",
        "            # Store experience in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # If done, finish trajectory and reset environment\n",
        "            if done or t == steps_per_epoch - 1:\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "                buffer.finish_trajectory(last_value)\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        (\n",
        "            observation_buffer,\n",
        "            action_buffer,\n",
        "            advantage_buffer,\n",
        "            return_buffer,\n",
        "            logprobability_buffer,\n",
        "        ) = buffer.get()\n",
        "\n",
        "        # Update policy with PPO\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:\n",
        "                break\n",
        "\n",
        "        # Update value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1} - Mean Return: {sum_return / num_episodes}, Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "# Start training PPO agent\n",
        "train_ppo(agent=None, env=env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "GuQeIMgY-JxJ",
        "outputId": "2bd01396-3081-4d32-d2a4-a1b198faa996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'episode_return' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4b3c70fadee9>\u001b[0m in \u001b[0;36m<cell line: 204>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;31m# Start training PPO agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-4b3c70fadee9>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(agent, env, epochs)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mobservation_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mepisode_length\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'episode_return' referenced before assignment"
          ]
        }
      ]
    }
  ]
}