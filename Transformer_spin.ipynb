{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pntMn5SI7jaY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "6edd3b11-75f0-4041-af76-2094e80f114f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Buffer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5dd135711ec0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mobservation_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mpolicy_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mvalue_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Buffer' is not defined"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from tensorflow.keras import layers, Model\n",
        "import gym\n",
        "\n",
        "# Create CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 1000\n",
        "steps_per_epoch = 2048\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01\n",
        "clip_ratio = 0.2\n",
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "batch_size = 64\n",
        "render = False\n",
        "\n",
        "# Initialize buffers and optimizers\n",
        "observation_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "buffer = Buffer(observation_dim, size=steps_per_epoch)\n",
        "policy_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "value_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "\n",
        "# Seed for reproducibility\n",
        "seed_generator = tf.random.Generator.from_seed(1337)\n",
        "\n",
        "# Transformer Model Definitions\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(units, activation='relu')(x)\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "def build_transformer_critic(input_shape, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "# Initialize actor and critic models\n",
        "actor = build_transformer_actor(input_shape=(observation_dim,), num_actions=num_actions)\n",
        "critic = build_transformer_critic(input_shape=(observation_dim,))\n",
        "\n",
        "# Function for computing log-probabilities\n",
        "def logprobabilities(logits, actions):\n",
        "    logprobabilities_all = tf.nn.log_softmax(logits, axis=-1)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(actions, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "# Sample action from the actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)  # Actor produces logits\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)  # Sample action\n",
        "    return logits, action\n",
        "\n",
        "# PPO Policy Update (using clipped objective)\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = actor(observation_buffer)  # Actor produces logits\n",
        "        action_probs = tf.nn.softmax(logits)  # Get the action probabilities\n",
        "        current_logprob = logprobabilities(logits, action_buffer)  # Log probability of taken actions\n",
        "\n",
        "        ratio = tf.exp(current_logprob - logprobability_buffer)  # Policy ratio (current / old)\n",
        "        min_advantage = tf.where(\n",
        "            advantage_buffer > 0,\n",
        "            (1 + clip_ratio) * advantage_buffer,\n",
        "            (1 - clip_ratio) * advantage_buffer\n",
        "        )\n",
        "\n",
        "        # PPO clipped objective\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    # Calculate KL divergence for monitoring\n",
        "    kl = tf.reduce_mean(logprobability_buffer - current_logprob)\n",
        "    return kl\n",
        "\n",
        "# PPO Value Function Update (MSE loss)\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_predictions = critic(observation_buffer)  # Critic output\n",
        "        value_loss = tf.reduce_mean(tf.square(return_buffer - value_predictions))  # MSE loss\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "# Discounted cumulative sums for advantages and returns\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "# Buffer Class for storing trajectories\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (\n",
        "            self.observation_buffer,\n",
        "            self.action_buffer,\n",
        "            self.advantage_buffer,\n",
        "            self.return_buffer,\n",
        "            self.logprobability_buffer,\n",
        "        )\n",
        "\n",
        "# Training Loop\n",
        "def train_ppo(agent, env, epochs=1000, steps_per_epoch=2048, train_policy_iterations=80, train_value_iterations=80, target_kl=0.01):\n",
        "    state = env.reset()\n",
        "    episode_return = 0\n",
        "    episode_length = 0\n",
        "    num_episodes = 0\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "\n",
        "        # Iterate over the steps of each epoch\n",
        "        for t in range(steps_per_epoch):\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            # Reshape observation for transformer models\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)  # Sample action from the actor\n",
        "            observation_new, reward, done, _, _ = env.step(action[0].numpy())  # Take step in the environment\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of action from the critic\n",
        "            value_t = critic(observation)  # Get value estimate from the critic\n",
        "            logprobability_t = logprobabilities(logits, action)  # Log-probability for action\n",
        "\n",
        "            # Store data in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # Finish trajectory if terminal state reached\n",
        "            if done or (t == steps_per_epoch - 1):\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))  # Last value from the critic\n",
        "                buffer.finish_trajectory(last_value)  # Finish trajectory\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation, _ = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer = buffer.get()\n",
        "\n",
        "        # Update the policy\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:  # Early stopping based on KL divergence\n",
        "                break\n",
        "\n",
        "        # Update the value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print stats for the epoch\n",
        "        print(f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "# Train PPO Agent on CartPole\n",
        "train_ppo(agent=None, env=env)  # Pass the environment and agent to the training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import gym\n",
        "import scipy.signal\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lam = 0.95  # GAE lambda\n",
        "clip_ratio = 0.2  # PPO clipping ratio\n",
        "epochs = 1000\n",
        "steps_per_epoch = 4000\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01  # Early stopping based on KL divergence\n",
        "\n",
        "# Buffer for storing experiences\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (self.observation_buffer, self.action_buffer, self.advantage_buffer, self.return_buffer, self.logprobability_buffer)\n",
        "\n",
        "\n",
        "# Discounted cumulative sum for rewards\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "# Transformer Actor model\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "# Transformer Critic model\n",
        "def build_transformer_critic(input_shape, units=256, num_heads=2, num_layers=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LayerNormalization()(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "\n",
        "# Log-probabilities computation for discrete actions\n",
        "def logprobabilities(logits, a):\n",
        "    logprobabilities_all = tf.math.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "    return logits, action\n",
        "\n",
        "\n",
        "# PPO Policy Gradient Update\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ratio = tf.exp(logprobabilities(actor(observation_buffer), action_buffer) - logprobability_buffer)\n",
        "        min_advantage = tf.where(advantage_buffer > 0, (1 + clip_ratio) * advantage_buffer, (1 - clip_ratio) * advantage_buffer)\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(actor(observation_buffer), action_buffer))\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Value Function Update\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "\n",
        "# Training loop for PPO agent\n",
        "def train_ppo(agent, env, epochs=epochs):\n",
        "    observation_shape = env.observation_space.shape\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Initialize models, optimizers, and buffers\n",
        "    global actor, critic, policy_optimizer, value_optimizer\n",
        "    actor = build_transformer_actor(observation_shape, num_actions)\n",
        "    critic = build_transformer_critic(observation_shape)\n",
        "    policy_optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "    value_optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    buffer = Buffer(observation_shape[0], size=steps_per_epoch)\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "        observation = env.reset()\n",
        "\n",
        "        for t in range(steps_per_epoch):\n",
        "            # Sample action and take a step in the environment\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)\n",
        "            observation_new, reward, done, _, _ = env.step(action.numpy()[0])\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of the action\n",
        "            value_t = critic(observation)\n",
        "            logprobability_t = logprobabilities(logits, action)\n",
        "\n",
        "            # Store experience in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # If done, finish trajectory and reset environment\n",
        "            if done or t == steps_per_epoch - 1:\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "                buffer.finish_trajectory(last_value)\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        (\n",
        "            observation_buffer,\n",
        "            action_buffer,\n",
        "            advantage_buffer,\n",
        "            return_buffer,\n",
        "            logprobability_buffer,\n",
        "        ) = buffer.get()\n",
        "\n",
        "        # Update policy with PPO\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:  # Early stopping based on KL divergence\n",
        "                break\n",
        "\n",
        "        # Update value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print stats for the epoch\n",
        "        print(f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "\n",
        "# Train the PPO agent on CartPole environment\n",
        "train_ppo(agent=None, env=env)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "nfPzbPUl9xzq",
        "outputId": "169b224e-89aa-4837-dd9b-6b6abe33772a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"global_average_pooling1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36m<cell line: 199>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Train the PPO agent on CartPole environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(agent, env, epochs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Initialize models, optimizers, and buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mpolicy_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b34f93220e3a>\u001b[0m in \u001b[0;36mbuild_transformer_actor\u001b[0;34m(input_shape, num_actions, num_heads, num_layers, units)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_last_axis_squeeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling1d\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import gym\n",
        "import scipy.signal\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "observation_shape = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "print(f\"observation_shape is {observation_shape}\")\n",
        "\n",
        "print(f\"num_actions is {num_actions}\")\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lam = 0.95  # GAE lambda\n",
        "clip_ratio = 0.2  # PPO clipping ratio\n",
        "epochs = 1000\n",
        "steps_per_epoch = 4000\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01  # Early stopping based on KL divergence\n",
        "\n",
        "# Initialize episode return and length\n",
        "episode_return = 0\n",
        "episode_length = 0\n",
        "\n",
        "# Buffer for storing experiences\n",
        "class Buffer:\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        self.observation_buffer = np.zeros((size, observation_dimensions), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(deltas, self.gamma * self.lam)\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(rewards, self.gamma)[:-1]\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = np.mean(self.advantage_buffer), np.std(self.advantage_buffer)\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (self.observation_buffer, self.action_buffer, self.advantage_buffer, self.return_buffer, self.logprobability_buffer)\n",
        "\n",
        "\n",
        "# Discounted cumulative sum for rewards\n",
        "def discounted_cumulative_sums(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "# Transformer Actor model\n",
        "#To do - explain below transformers in ReadMe\n",
        "def build_transformer_actor(input_shape, num_actions, num_heads=2, num_layers=2, units=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Reshape((1, input_shape[0]))(inputs)  # Reshape to (batch_size, 1, features)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pool across the sequence dimension\n",
        "    logits = layers.Dense(num_actions)(x)\n",
        "    return Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "# Transformer Critic model\n",
        "def build_transformer_critic(input_shape, units=256, num_heads=2, num_layers=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Reshape((1, input_shape[0]))(inputs)  # Reshape to (batch_size, 1, features)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=units)(x, x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pool across the sequence dimension\n",
        "    value = layers.Dense(1)(x)\n",
        "    return Model(inputs=inputs, outputs=value)\n",
        "\n",
        "\n",
        "# Log-probabilities computation for discrete actions\n",
        "def logprobabilities(logits, a):\n",
        "    logprobabilities_all = tf.math.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, num_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    logits = actor(observation)\n",
        "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "    return logits, action\n",
        "\n",
        "\n",
        "# PPO Policy Gradient Update\n",
        "@tf.function\n",
        "def train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ratio = tf.exp(logprobabilities(actor(observation_buffer), action_buffer) - logprobability_buffer)\n",
        "        min_advantage = tf.where(advantage_buffer > 0, (1 + clip_ratio) * advantage_buffer, (1 - clip_ratio) * advantage_buffer)\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantage_buffer, min_advantage))\n",
        "\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(actor(observation_buffer), action_buffer))\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Value Function Update\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n",
        "\n",
        "\n",
        "# Training loop for PPO agent\n",
        "def train_ppo(agent, env, epochs=epochs):\n",
        "    #observation_shape = env.observation_space.shape\n",
        "    #num_actions = env.action_space.n\n",
        "    # ^ Placing above declarations at beginning -- out of scope as it stands.\n",
        "\n",
        "    # Initialize models, optimizers, and buffers\n",
        "    global actor, critic, policy_optimizer, value_optimizer\n",
        "    actor = build_transformer_actor(observation_shape, num_actions)\n",
        "    critic = build_transformer_critic(observation_shape)\n",
        "    policy_optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "    value_optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    buffer = Buffer(observation_shape[0], size=steps_per_epoch)\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(epochs):\n",
        "        sum_return = 0\n",
        "        sum_length = 0\n",
        "        num_episodes = 0\n",
        "        observation = env.reset()\n",
        "\n",
        "        episode_return = 0\n",
        "        episode_length = 0\n",
        "        for t in range(steps_per_epoch):\n",
        "            # Sample action and take a step in the environment\n",
        "            observation = observation.reshape(1, -1)\n",
        "            logits, action = sample_action(observation)\n",
        "            observation_new, reward, done, info = env.step(action.numpy()[0])\n",
        "            episode_return += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Get value and log-probability of the action\n",
        "            value_t = critic(observation)\n",
        "            logprobability_t = logprobabilities(logits, action)\n",
        "\n",
        "            # Store experience in the buffer\n",
        "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "            # Update observation\n",
        "            observation = observation_new\n",
        "\n",
        "            # If done, finish trajectory and reset environment\n",
        "            if done or t == steps_per_epoch - 1:\n",
        "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "                buffer.finish_trajectory(last_value)\n",
        "                sum_return += episode_return\n",
        "                sum_length += episode_length\n",
        "                num_episodes += 1\n",
        "                observation = env.reset()\n",
        "                episode_return, episode_length = 0, 0\n",
        "\n",
        "        # Get data from the buffer\n",
        "        (\n",
        "            observation_buffer,\n",
        "            action_buffer,\n",
        "            advantage_buffer,\n",
        "            return_buffer,\n",
        "            logprobability_buffer,\n",
        "        ) = buffer.get()\n",
        "\n",
        "        # Update policy with PPO\n",
        "        for _ in range(train_policy_iterations):\n",
        "            kl = train_policy(observation_buffer, action_buffer, logprobability_buffer, advantage_buffer)\n",
        "            if kl > 1.5 * target_kl:\n",
        "                break\n",
        "\n",
        "        # Update value function\n",
        "        for _ in range(train_value_iterations):\n",
        "            train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1} - Mean Return: {sum_return / num_episodes}, Mean Length: {sum_length / num_episodes}\")\n",
        "\n",
        "\n",
        "# Start training PPO agent\n",
        "train_ppo(agent=None, env=env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "GuQeIMgY-JxJ",
        "outputId": "5f25d3c2-c13a-4118-bc45-c7b8fbb56e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_shape is (4,)\n",
            "num_actions is 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py:291: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return int(self._numpy())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Mean Return: 43.01075268817204, Mean Length: 43.01075268817204\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6388d3d527e6>\u001b[0m in \u001b[0;36m<cell line: 214>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# Start training PPO agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6388d3d527e6>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(agent, env, epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# Get value and log-probability of the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mvalue_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mlogprobability_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogprobabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;31m# be called multiple times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0meager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m_clear_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_layers\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3306\u001b[0;31m         for m in self._flatten_modules(\n\u001b[0m\u001b[1;32m   3307\u001b[0m             \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_modules\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3332\u001b[0m                 \u001b[0mtrackable_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrackable_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_object_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3334\u001b[0;31m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3335\u001b[0m                 \u001b[0mseen_object_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}